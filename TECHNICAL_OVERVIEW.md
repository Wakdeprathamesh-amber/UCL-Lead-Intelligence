# ğŸ“˜ Technical Overview - Quick Reference

> **Fast reference guide to understanding the UCL Lead Intelligence AI system**

---

## ğŸ¯ What This System Does

**In Plain English:**
A chatbot that lets UCL admins ask natural language questions about their student leads and get instant, accurate answers backed by real data.

**Technical:**
A hybrid RAG + MCP architecture that intelligently routes queries between structured database queries (SQLite) and semantic search (ChromaDB) using GPT-4o as the orchestration layer.

---

## ğŸ—ï¸ Architecture in 60 Seconds

### The Stack

```
Frontend:  Streamlit (Python web app)
Backend:   LangChain + GPT-4o (AI orchestration)
Databases: SQLite (structured) + ChromaDB (vectors)
APIs:      OpenAI (embeddings + chat completion)
```

### Two Data Paths

**Path 1: MCP (Structured Queries)**
```
User â†’ GPT-4o â†’ SQL Query â†’ SQLite â†’ Exact Results
```
*Use for: Filters, lookups, statistics*

**Path 2: RAG (Semantic Search)**
```
User â†’ GPT-4o â†’ Embedding â†’ ChromaDB â†’ Relevant Context
```
*Use for: Themes, concerns, conversations*

---

## ğŸ—„ï¸ Database Comparison

| Feature | SQLite | ChromaDB |
|---------|--------|----------|
| **Purpose** | Structured data | Semantic search |
| **Data Type** | Relational tables | Vector embeddings |
| **Query Type** | SQL | Similarity search |
| **Best For** | Exact matches | Finding meaning |
| **Speed** | 10-100ms | 150-300ms |
| **Accuracy** | 100% | ~85% relevance |
| **Size** | 500 KB (14 leads) | 5 MB (24 docs) |

### SQLite Tables (5 total)
1. `leads` - Main lead info
2. `lead_requirements` - Budget, dates, preferences
3. `lead_objections` - Concerns raised
4. `lead_tasks` - Action items
5. `rag_documents` - Text for embeddings

### ChromaDB Collection (1 total)
- `lead_conversations` - 24 embedded documents
- Embedding model: `text-embedding-3-small`
- Dimensions: 1536
- Distance metric: Cosine similarity

---

## ğŸ”€ Query Routing

### How GPT-4o Decides

```python
# Simplified pseudo-code

if query.has_exact_criteria():
    use_sqlite()  # MCP path
    
elif query.about_meaning_or_themes():
    use_chromadb()  # RAG path
    
elif query.needs_both():
    use_sqlite() + use_chromadb()  # Hybrid
```

### Real Examples

| Query | Route | Why |
|-------|-------|-----|
| "Budget < Â£400" | SQLite | Exact filter |
| "What concerns?" | ChromaDB | Semantic meaning |
| "Why choose X?" | Both | Facts + context |
| "How many leads?" | SQLite | Count/aggregate |
| "Worried about?" | ChromaDB | Theme search |

---

## âš¡ Performance

### Query Response Times

```
Component              Time
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT-4o reasoning:      1-2s   (constant)
SQLite query:          10-100ms
Embedding generation:  100ms  (OpenAI API)
ChromaDB search:       50-200ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total (MCP):          ~1.5s
Total (RAG):          ~2.5s
Total (Hybrid):       ~3.5s
```

### Bottlenecks
1. **GPT-4o API calls** - Largest time consumer
2. **OpenAI embedding API** - For RAG queries
3. **Network latency** - API calls to OpenAI

**Database queries are fast!** (<100ms each)

---

## ğŸ”§ Key Components

### 1. Data Ingestion (`src/data_ingestion.py`)
- Parses CSV lead data
- Creates SQLite tables
- Extracts text for RAG

### 2. Query Tools (`src/query_tools.py`)
- MCP layer implementation
- 7 tools for structured queries
- Direct SQLite access

### 3. RAG System (`src/rag_system.py`)
- Vector embedding generation
- ChromaDB management
- Semantic search tools

### 4. AI Agent (`src/ai_agent.py`)
- LangChain orchestration
- Query routing logic
- Tool selection
- Response formatting

### 5. Web App (`app.py`)
- Streamlit interface
- Chat UI
- Dashboard KPIs

---

## ğŸ“Š Data Flow Examples

### Example 1: Factual Query

```
"Show leads with budget < Â£400"
â”‚
â”œâ”€â–º GPT-4o: This needs exact filtering
â”‚
â”œâ”€â–º Call: filter_leads(budget_max=400)
â”‚
â”œâ”€â–º SQL: SELECT * FROM leads WHERE budget_max <= 400
â”‚
â””â”€â–º Response: "Found 1 lead: Laia (Â£395)"
```

### Example 2: Semantic Query

```
"What are students worried about?"
â”‚
â”œâ”€â–º GPT-4o: This needs semantic understanding
â”‚
â”œâ”€â–º Call: semantic_search("students worried")
â”‚
â”œâ”€â–º Generate embedding [1536 dims]
â”‚
â”œâ”€â–º ChromaDB: Find similar vectors
â”‚
â”œâ”€â–º Returns: 5 most relevant conversations
â”‚
â””â”€â–º Response: "Top concerns: Budget (3), Safety (2)..."
```

### Example 3: Hybrid Query

```
"Why did Laia choose this property?"
â”‚
â”œâ”€â–º GPT-4o: Needs facts + context
â”‚
â”œâ”€â–º Call: get_lead_by_id("Laia") [MCP]
â”‚   â””â”€â–º Returns: Budget Â£395, Studio, London
â”‚
â”œâ”€â–º Call: semantic_search("Laia decision") [RAG]
â”‚   â””â”€â–º Returns: Safety concerns, transport needs
â”‚
â””â”€â–º Response: "Chose because: budget fit, safety, 
              good transport (sources: both DBs)"
```

---

## ğŸ“ How RAG Works

### 1. Embedding Generation
```
Text: "Student concerned about budget constraints"
      â†“
OpenAI API (text-embedding-3-small)
      â†“
Vector: [0.123, -0.456, 0.789, ... Ã— 1536]
```

### 2. Storage
```
ChromaDB stores:
  â€¢ Vector (1536 floats)
  â€¢ Original text
  â€¢ Metadata (lead_id, status, type)
```

### 3. Search
```
Query: "budget concerns"
      â†“
Convert to vector
      â†“
Find nearest vectors (cosine similarity)
      â†“
Return top K matches with metadata
```

### 4. Why It Works
- **Semantic understanding**: "expensive" â‰ˆ "high cost" â‰ˆ "budget issue"
- **Context preserved**: Full conversation text stored
- **Fast retrieval**: Vector math is quick
- **Relevance ranking**: Distance scores show relevance

---

## ğŸ”‘ Key Design Decisions

### 1. Why Hybrid (MCP + RAG)?
- **MCP alone**: Can't understand themes/meaning
- **RAG alone**: Can't do exact filters accurately
- **Both together**: Best of both worlds

### 2. Why SQLite?
- âœ… No server setup
- âœ… File-based (easy deployment)
- âœ… Fast for small datasets
- âœ… Perfect for POC

**Trade-off**: Won't scale to 100k+ leads (use PostgreSQL then)

### 3. Why ChromaDB?
- âœ… File-based (no server)
- âœ… Built for embeddings
- âœ… Good for POC
- âœ… Easy to use

**Trade-off**: Won't scale to millions of docs (use Pinecone then)

### 4. Why GPT-4o?
- âœ… Best reasoning capabilities
- âœ… Function calling (tool selection)
- âœ… Natural language understanding
- âœ… Context synthesis

**Trade-off**: Expensive at scale (consider caching/fine-tuning)

---

## ğŸš€ Scaling Considerations

### Current POC (14 leads)
```
âœ“ SQLite:   Perfect
âœ“ ChromaDB: Perfect
âœ“ Cost:     ~$5-10/month
âœ“ Speed:    Sub-3s responses
```

### Production (1000+ leads)
```
â†’ PostgreSQL (better concurrency)
â†’ Pinecone/Weaviate (managed vector DB)
â†’ Caching layer (Redis)
â†’ Load balancing
â†’ Cost: ~$100-200/month per tenant
```

### Enterprise (10,000+ leads, multi-tenant)
```
â†’ Kubernetes deployment
â†’ Database sharding
â†’ CDN for static assets
â†’ Advanced caching
â†’ Fine-tuned models (reduce API costs)
â†’ Cost: $500-1000/month
```

---

## ğŸ“ˆ Metrics & Monitoring

### What to Track

**Performance:**
- Query response time
- Database query time
- API call latency
- Error rates

**Usage:**
- Queries per day
- Tool usage breakdown (MCP vs RAG)
- Popular query types
- User satisfaction

**Cost:**
- OpenAI API costs (GPT-4o + embeddings)
- Compute resources
- Storage costs

**Quality:**
- Answer accuracy (user feedback)
- Source citation rate
- Tool selection accuracy

---

## ğŸ”’ Security & Privacy

### Current POC
- âœ… Local-only
- âœ… No authentication
- âœ… No data sharing

### Production Needs
- ğŸ” User authentication
- ğŸ” Role-based access control
- ğŸ” Data encryption at rest
- ğŸ” Audit logging
- ğŸ” GDPR compliance
- ğŸ” Rate limiting

---

## ğŸ“š Documentation Guide

**For different audiences:**

### Developers
- ğŸ“– `ARCHITECTURE.md` - Deep technical details
- ğŸ“– `QUERY_FLOW_DIAGRAMS.md` - Visual flow examples
- ğŸ“– `TECHNICAL_OVERVIEW.md` - This file!
- ğŸ“– `src/` directory - Implementation code

### Users/Admins
- ğŸ“– `README.md` - Project overview
- ğŸ“– `QUICKSTART.md` - Get started fast
- ğŸ“– `DEMO_SCRIPT.md` - Demo guidance

### Stakeholders
- ğŸ“– `PROJECT_STATUS.md` - Current state
- ğŸ“– `DEMO_SCRIPT.md` - Presentation flow

---

## ğŸ› ï¸ Common Tasks

### Add a New MCP Tool
```python
# In src/query_tools.py
def my_new_query(self, param):
    conn = self._get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT ... WHERE ?", (param,))
    return cursor.fetchall()

# In src/ai_agent.py
Tool(
    name="my_new_query",
    func=lambda x: self.query_tools.my_new_query(x),
    description="What this tool does..."
)
```

### Add New Lead Data
```python
# Re-run ingestion
python src/data_ingestion.py

# Re-create embeddings
python src/rag_system.py
```

### Change Embedding Model
```python
# In src/rag_system.py
self.embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",  # More accurate but costly
    # or "text-embedding-3-small"     # Faster, cheaper
)
```

---

## ğŸ› Debugging Guide

### Query Not Working?

**Check:**
1. API key valid? (`.env` file)
2. Database has data? (`sqlite3 data/leads.db`)
3. Embeddings created? (`ls data/chroma_db/`)
4. Error in logs? (Streamlit terminal output)

### Slow Responses?

**Profile:**
1. GPT-4o calls (1-2s) - normal
2. Database queries (>1s) - investigate
3. Embedding generation (>500ms) - check OpenAI API
4. Network issues - check internet

### Wrong Results?

**Debug:**
1. Check which tool was used (look at sources)
2. For MCP: Verify SQL query logic
3. For RAG: Check relevance scores (<0.7 is good)
4. Review GPT-4o reasoning (add verbose=True)

---

## ğŸ¯ Quick Reference

### File Structure
```
â”œâ”€â”€ app.py                    # Main Streamlit app
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py     # CSV â†’ SQLite
â”‚   â”œâ”€â”€ query_tools.py        # MCP layer
â”‚   â”œâ”€â”€ rag_system.py         # RAG layer
â”‚   â””â”€â”€ ai_agent.py           # Orchestration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ leads.db              # SQLite database
â”‚   â””â”€â”€ chroma_db/            # Vector store
â””â”€â”€ Data/
    â””â”€â”€ UCL Leads Data.csv    # Source data
```

### Key Commands
```bash
# Run app
streamlit run app.py

# View database
sqlite3 data/leads.db
# or
open http://localhost:8080  # (sqlite-web)

# Re-ingest data
python src/data_ingestion.py

# Re-create embeddings
python src/rag_system.py

# Test agent
python src/ai_agent.py
```

### Environment Variables
```bash
# .env file
OPENAI_API_KEY=sk-...
```

---

## ğŸ¤” FAQ

**Q: Why not just use RAG for everything?**  
A: RAG can't do exact filters accurately. "Budget < 400" might return "budget around 400-ish" instead of precise results.

**Q: Why not just use SQL for everything?**  
A: SQL can't understand meaning. "What worries students?" has no SQL equivalent without predefined categories.

**Q: How accurate is the RAG search?**  
A: ~85-90% relevance for well-formed queries. Quality depends on embedding model and document quality.

**Q: Can I add more data?**  
A: Yes! Just update the CSV and re-run ingestion + embedding scripts.

**Q: How much does it cost to run?**  
A: POC: ~$5-10/month. Production: ~$100-200/month. Depends on query volume.

**Q: Will it work with 10,000 leads?**  
A: Need to switch to PostgreSQL + Pinecone, but architecture stays the same!

---

## ğŸ“ Support

- **Architecture questions**: See `ARCHITECTURE.md`
- **Flow diagrams**: See `QUERY_FLOW_DIAGRAMS.md`
- **Usage help**: See `QUICKSTART.md`
- **Demo prep**: See `DEMO_SCRIPT.md`

---

**Built with** â¤ï¸ **for UCL Lead Intelligence**

